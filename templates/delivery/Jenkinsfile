// Used to skip the Snyk scan when a previous step fails
def skip_snyk_scan = false

pipeline {
  parameters {
    string(name: 'git_repository', description: 'The URL of the Git repository to clone when building the container image.')
    string(name: 'git_commit', description: 'The commit hash or branch name to checkout when building the container image.')
    string(name: 'git_credentials', description: 'The ID of the Jenkins credentials to use when cloning the Git repository.')
    booleanParam(name: 'image_push_enabled', defaultValue: "${default_image_push_enabled ?: false}", description: 'Enable the image push step.')
    string(name: 'image', defaultValue: "${default_image ?: 'my-app'}", description: 'The fully qualified container image name.')
    string(name: 'tag', defaultValue: "${default_tag ?: 'latest'}", description: 'Container image tag.')

    booleanParam(name: 'update_latest', defaultValue: "${default_update_latest ?: false}", description: 'When true, update the latest tag to point to the newly built image.')
    string(name: 'build_args', defaultValue: "${default_build_args ?: '{}'}", description: 'A JSON object of build arguments to pass to the container build process.')
    string(name: 'build_dir', defaultValue: "${default_build_dir ?: '.'}", description: 'The directory to use as the build context for the container build.')
    string(name: 'dockerfile', defaultValue: "${default_dockerfile ?: 'Dockerfile'}", description: 'The path to the Dockerfile to use for the container build.')
    string(name: 'build_target', defaultValue: "${default_build_target ?: ''}", description: 'The build target to use when building the container image (for use with multi-stage Dockerfiles).')
    string(name: 'copy_artifacts_job_name', defaultValue: '', description: 'Optional, name of the Jenkins job from which to copy build artifacts that may be required by the Dockerfile.')
    string(name: 'copy_artifacts_build_number', defaultValue: '', description: 'Optional, specific build number of artifacts to copy.')
    string(name: 'copy_artifacts_filter', defaultValue: '', description: 'Optional, filemask to identify build artifacts to copy.')
    string(name: 'platform', defaultValue: "${default_platform ?: ''}", description: 'A comma separated list of platforms to build the container image for (e.g., linux/amd64,linux/arm64).')
    choice(name: 'log_level', choices: ['info', 'panic', 'fatal', 'error', 'warn', 'debug', 'trace'], description: 'The log level to use for the container build process.')
    string(name: 'kaniko_memory_limit', defaultValue: "${default_kaniko_memory_limit ?: '1Gi'}", description: "Kaniko memory limit input for those larger builds.")

    booleanParam(name: 'enable_cache', defaultValue: "${default_enable_cache ?: false}", description: 'Enable Kaniko image build cache.')

    booleanParam(name: 'enable_snyk', defaultValue: "${default_enable_snyk ?: false}", description: 'Enable the Snyk container image vulnerability scan step.')
    string(name: 'snyk_project_name', defaultValue: "${default_snyk_project_name ?: 'my-app'}", description: 'The name of the Snyk project to associate the container image scan with.')
    string(name: 'vulnerability_severity_threshold', defaultValue: "${default_vulnerability_severity_threshold ?: 'high'}", description: 'The minimum severity level of vulnerabilities which will cause the build to fail if detected (options: low, medium, high, critical).')

    booleanParam(name: 'continue_on_image_scan_failure', defaultValue: "${default_continue_on_image_scan_failure ?: false}", description: 'When set to true, the pipeline will continue to the image publish step even if either of the malware scan or vulnerability scan fails for any reason.')
  }

  agent {
    kubernetes {
      yaml """
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          com.cloudbees.sidecar-injector/inject: no
      spec:
        restartPolicy: Never
        containers:
        - name: fetch
          image: artifactory.cloud.cms.gov/docker/alpine:3
          command: ['tail', '-f', '/dev/null']
        - name: kaniko
          # Note: it is necessary to use the -debug version of the kaniko image because it includes
          # basic shell utilities like tail which is required to keep the container running
          image: artifactory.cloud.cms.gov/docker/kaniko-project/executor:v1.23.2-debug
          command: ['tail', '-f', '/dev/null']
          resources:
            limits:
              memory: "${params.kaniko_memory_limit}"
        - name: clamav
          image: artifactory.cloud.cms.gov/docker/clamav/clamav:latest
          command: ['tail', '-f', '/dev/null']
          resources:
            limits:
              memory: "5Gi"
            requests:
              memory: "3Gi"
        - name: skopeo
          # TODO: setup a artifactory based mirror for quay.io/skopeo/stable
          image: quay.io/skopeo/stable:v1.15
          command: ['tail', '-f', '/dev/null']
        - name: snyk
          image: artifactory.cloud.cms.gov/docker/snyk/snyk:alpine
          command: ['tail', '-f', '/dev/null']
        - name: crane
          image: gcr.io/go-containerregistry/crane/debug:latest
          command: ['tail', '-f', '/dev/null']
      """
    }
  }

  options {
    // Allow other pipelines to copy the artifacts produced by this pipeline
    copyArtifactPermission('*')
    // Automatically discard old builds
    buildDiscarder(logRotator(daysToKeepStr: "${build_retention_days}", numToKeepStr: "${build_retention_count}"))
  }

  stages {
    stage('Build Image') {
      steps {
        container('fetch') {
          checkout scmGit(
            branches: [[ name: "${params.git_commit}" ]],
            userRemoteConfigs: [[
              url: "${params.git_repository}",
              credentialsId: "${params.git_credentials}",
              refspec: "+${params.git_commit}:refs/remotes/origin/HEAD"
            ]],
            extensions: [
              cloneOption(shallow: true, honorRefspec: true, noTags: true),
              submodule(recursiveSubmodules: true, shallow: true)
            ]
          )
        }
        container('kaniko') {
          script {
            def arguments = [
              "--context=${params.build_dir}",
              "--dockerfile=${params.dockerfile}",
              "--destination=${params.image}:${params.tag}",
              "--verbosity=${params.log_level}",
              '--no-push',
              '--tar-path=./image.tar'
            ]

            if (params.build_target) {
              echo "build_target (${params.build_target.getClass()}): ${params.build_target}"
              echo "default_build_target: ${default_build_target}"
              arguments.add("--target=${params.build_target}")
            }

            if (params.platform) {
              arguments.add("--custom-platform=${params.platform}")
            }

            if (params.build_args) {
              def argMap = new groovy.json.JsonSlurper().parseText(params.build_args)
              argMap.each { key, value ->
                arguments.add("--build-arg=${key}='${value}'")
              }
            }

            if (params.copy_artifacts_job_name && params.copy_artifacts_build_number && params.copy_artifacts_filter) {
              copyArtifacts projectName: params.copy_artifacts_job_name, filter: params.copy_artifacts_filter, selector: specific(params.copy_artifacts_build_number)
            }

            if (params.enable_cache) {
              arguments.add("--cache=${params.enable_cache}")
              arguments.add("--cache-repo=${params.image}-cache")
              arguments.add("--compressed-caching=false")
              arguments.add("--cache-dir=/kaniko/cache")
              arguments.add("--cache-run-layers")
              arguments.add("--cache-copy-layers")
            }

            // Authentication by copying a docker config JSON to config directory
            withCredentials([file(credentialsId: "${image_registry_auth_json}", variable: 'AUTH_JSON_FILE')]) {
              echo 'export image registry auth json to /kaniko/.docker/config.json'
              sh 'mkdir -p /kaniko/.docker'
              sh 'mv "$AUTH_JSON_FILE" /kaniko/.docker/config.json'

              if ("${enable_ansi_colors}" == "true") {
                ansiColor('xterm') {
                  sh "/kaniko/executor ${arguments.join(' ')}"
                }
              } else {
                sh "/kaniko/executor ${arguments.join(' ')}"
              }
            }
          }
        }
      }
    }

    stage('Image Scan') {
      parallel {
        stage('Vulnerability Scan') {
          when {
            expression { params.enable_snyk }
          }
          steps {
            container('skopeo') {
              script {
                try {
                  // The image file that Kaniko produces is not compatible with Snyk (https://github.com/GoogleContainerTools/kaniko/issues/1976)
                  // By "copying" it with Skopeo, we create a new image file that Snyk can scan
                  sh 'skopeo copy docker-archive:image.tar docker-archive:skopeo-image.tar'
                } catch(err) {
                  if (params.continue_on_image_scan_failure) {
                    echo "Failed to copy the image file with skopeo. Skipping the Snyk scan."
                    skip_snyk_scan = true
                  } else {
                    error "Failed to copy the image file with skopeo."
                  }
                }
              }
            }
            container('snyk') {
              script {
                try {
                  if (skip_snyk_scan) {
                    error "Skipping the Snyk scan due to a previous failure."
                  }

                  def image_file_name = "'${params.image.replace(':', '_')}:${params.tag.replace(':', '_')}'".replace('/', '_')
                  sh "ln skopeo-image.tar ${image_file_name}"
                  def args = [
                    // Application vulnerabilities can be detected using `snyk test` as part of the SAST step
                    '--exclude-app-vulns',
                    "--severity-threshold=${params.vulnerability_severity_threshold}",
                    "--project-name=${params.snyk_project_name}",
                    "docker-archive:${image_file_name}"
                  ]
                  def sbom_ext = "${sbom_format}".tokenize('+')[1]
                  try {
                    withCredentials([string(credentialsId: "${snyk_token}", variable: 'SNYK_TOKEN')]) {
                      sh """
                        # Report vulnerabilities to the Snyk dashboard
                        snyk container monitor ${args.join(' ')}
                        # Note: use skopeo-image.tar because the snyk sbom command does not support
                        # filenames with colons.
                        snyk container sbom --format=${sbom_format} docker-archive:skopeo-image.tar > './snyk-sbom.${sbom_ext}'
                        # Report detected vulnerabilities to stdout and raise an error if
                        # vulnerabilities are found exceeding the specified threshold
                        snyk container test --json-file-output=snyk-vulnerabilities.json ${args.join(' ')}
                      """
                    }
                  } finally {
                    archiveArtifacts artifacts: 'snyk-sbom.*, snyk-vulnerabilities.json', allowEmptyArchive: true
                  }
                } catch(err) {
                  if (params.continue_on_image_scan_failure) {
                    catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                      error "Failed to scan the image with Snyk. Continuing pipeline execution."
                    }
                  } else {
                    echo "Snyk scan failed with error: ${err}"
                    error "Failed to scan the image with Snyk."
                  }
                }
              }
            }
          }
        }

        stage('Malware Scan') {
          steps {
            container('clamav') {
              script {
                try {
                  echo "Updating ClamAV database"
                  def freshclamExitCode = sh(script: "freshclam", returnStatus: true)
                  if (freshclamExitCode != 0) {
                    echo "Freshclam exited with code ${freshclamExitCode}"
                    error 'Failed to update ClamAV database.'
                  }
                  echo "ClamAV database updated successfully, running clamscan"
                  sh """
                    set -o pipefail
                    clamscan --infected --recursive --scan-archive=yes --max-filesize=500M --max-scansize=500M --stdout ./image.tar | tee virus-report.clamav.txt
                  """
                } catch(err) {
                  if (params.continue_on_image_scan_failure) {
                    catchError(buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
                      error "Failed to scan the image with ClamAV. Continuing pipeline execution."
                    }
                  } else {
                    error "Failed to scan the image with ClamAV."
                  }
                } finally {
                  archiveArtifacts artifacts: 'virus-report.clamav.txt', allowEmptyArchive: true
                }
              }
            }
          }
        }
      }
    }

    stage('Image Publish') {
      when {
        expression { params.image_push_enabled }
      }
      steps {
        container('crane') {
          script {
            // Authentication by copying a docker config JSON to config directory
            withCredentials([file(credentialsId: "${image_registry_auth_json}", variable: 'AUTH_JSON_FILE')]) {
              echo 'export image registry auth json to /root/.docker/config.json'
              sh 'mkdir -p /root/.docker'
              sh 'mv "$AUTH_JSON_FILE" /root/.docker/config.json'
              sh 'crane version'
              sh 'pwd && ls -laH'
              sh "crane push ./image.tar ${params.image}:${params.tag}"

              if (params.update_latest) {
                sh "crane push ./image.tar ${params.image}:latest"
              }
            }
          }
        }
      }
    }
  }
}
